"""
quick_try.py: used for quick tests to make sure everything works as expected
"""

from matplotlib import pyplot as plt
import numpy as np
import torch
import torch.nn.functional as F
from nltk.translate.bleu_score import sentence_bleu
from nltk.translate.meteor_score import single_meteor_score
from rouge import Rouge


def array_lengths():
    lengths = []
    target_tensor = [[2, 5, 3, 0, 0, 0, 0, 0], [2, 5, 6, 6, 6, 3, 0, 0, 0, 0]]
    for sentence in target_tensor:
        for idx in range(len(sentence)):
            if sentence[idx] == 3:
                lengths.append(idx)
                break

    print(lengths)


def graph():
    train = [1, 2, 3, 4, 5, 6, 34, 3]
    val = [1, 3, 4, 5, 6, 7, 6, 34]

    x = np.linspace(0, len(train), len(train))
    plt.plot(x, train, label="train")
    plt.plot(x, val, label="val")
    plt.title("Signs2Text - %s" % "str")
    plt.ylabel("CrossEntropyLoss")
    plt.xlabel("epochs")
    plt.legend()
    plt.show()


def string_formatting():
    test_float = 34.03
    test_float_1 = 444.033

    print("%10.4f" % test_float)
    print("%3.2f" % test_float_1)


def test_modulo():
    print(10 % -2)
    for number in range(20):
        print(10 % (number + 1))
        print("---")


def test_slicing():
    a = [666.558, 212.17, 0.927353, 674.446, 353.161, 0.779926, 570.547, 357.094, 0.659351, 568.586, 543.15, 0.704853,
         615.651, 404.075, 0.710361, 782.197, 351.238, 0.663412, 789.963, 541.26, 0.725472, 713.671, 396.239, 0.702378,
         680.256, 650.923, 0.50188, 607.858, 650.916, 0.454081, 0, 0, 0, 0, 0, 0, 744.973, 654.827, 0.413601, 0, 0, 0,
         0, 0, 0, 645.003, 194.405, 0.88849, 686.233, 194.356, 0.880671, 617.578, 216.01, 0.860892, 717.522, 210.176,
         0.832831, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    print(a)
    every_other = a[::3]
    print(every_other)
    b = a[:25:3]
    b.extend(a[45:55:3])
    print(b)


def self_attention():
    """
    try self-attention from peter bloemi:
    http://www.peterbloem.nl/blog/transformers
    :return:
    """
    # assume we have some tensor x with size (b, t, k)
    x = torch.randn(2, 3, 4, dtype=torch.double)
    print("x")
    print(x.size())
    print(x)

    x_T = x.transpose(1, 2)
    print("x_T")
    print(x_T.size())
    print(x_T)

    raw_weights = torch.bmm(x, x_T)
    print("raw_weights")
    print(raw_weights.size())
    print(raw_weights)

    weights = F.softmax(raw_weights, dim=2)
    print("weights")
    print(weights.size())
    print(weights)

    y = torch.bmm(weights, x)
    print("y")
    print(y.size())
    print(y)


def test_scores():
    reference = ["hi", "how", "are", "you", "today"]
    hypothesis = ["hi", "how", "are", "you"]
    ref_str = "hi how are you today"
    hyp_str = "hi how are you today"

    rouge = Rouge()

    bleu1_score = round(sentence_bleu([reference], hypothesis, weights=(1, 0, 0, 0)), 4)
    bleu2_score = round(sentence_bleu([reference], hypothesis, weights=(0.5, 0.5, 0, 0)), 4)
    bleu3_score = round(sentence_bleu([reference], hypothesis, weights=(0.33, 0.33, 0.33, 0)), 4)
    bleu4_score = round(sentence_bleu([reference], hypothesis, weights=(0.25, 0.25, 0.25, 0.25)), 4)
    meteor_score = round(single_meteor_score(ref_str, hyp_str), 4)
    rouge_score = round(rouge.get_scores(hyp_str, ref_str)[0]["rouge-l"]["f"], 4)
    # rouge_score = rouge.get_scores(ref_str, hyp_str)

    print('BLEU Cumulative 1-gram: %f' % bleu1_score)
    print('BLEU Cumulative 2-gram: %f' % bleu2_score)
    print('BLEU Cumulative 3-gram: %f' % bleu3_score)
    print('BLEU Cumulative 4-gram: %f' % bleu4_score)
    print('BLEU Cumulative 4-gram: %f' % bleu4_score)
    print('METEOR score: %f' % meteor_score)
    print('ROUGE-L F1 score score: %f' % rouge_score)
    # print(rouge_score[0]["rouge-l"]["f"])
    print(
        "src len: %4d | tgt len: %4d | b1 %5.2f | b2 %5.2f | b3 %5.2f | b4 %5.2f | meteor %5.2f | rouge %5.2f |" % (
            len(hypothesis), len(reference), bleu1_score, bleu2_score, bleu3_score,
            bleu4_score, meteor_score, rouge_score))


def test_graphs():
    fig = plt.figure()
    ax = plt.axes()

    train = [8.215386199951173, 8.087236785888672, 8.174390157063803, 8.009881337483725, 8.040818786621093,
             2.7026589711507163, 7.7964731852213545, 2.755674680074056, 7.422765731811523, 7.670327758789062,
             1.5455988883972167, 7.240325927734375, 2.4608167012532554, 7.87783940633138, 6.659430821736653,
             7.503168106079102, 1.3882229804992676, 5.966302871704102, 7.349739074707031, 1.3907354354858399,
             5.462225596110026, 0.9078583717346191, 2.270462989807129, 6.538099670410157, 1.380219300587972,
             2.023831049601237, 1.1596412658691406, 1.710752805074056, 5.728250503540039, 1.7432320912679036,
             5.258832931518555, 4.378861999511718, 3.3577274322509765, 3.503806304931641, 2.2260125478108725,
             2.4751285552978515, 1.8704136212666829, 6.096518198649089, 5.14922555287679, 1.0035561561584472,
             2.8797889709472657, 1.3531945546468098, 0.8781549135843912, 0.6561714808146158, 5.468982696533203,
             1.2807374795277913, 3.820391845703125, 2.3453144073486327, 1.2660767555236816, 3.710667928059896,
             1.3486720720926921, 1.9082903861999512, 0.6979142824808756, 3.413365936279297, 1.1261138916015625,
             2.997511863708496, 1.703589630126953, 1.2294619878133137, 0.525480310122172, 2.5931324005126952,
             3.343092918395996, 1.1278378168741863, 3.7538612365722654, 0.8337187767028809, 1.8591337203979492,
             0.8517416000366211, 0.8978434403737386, 1.977351760864258, 2.278752326965332, 1.1112377166748046,
             1.42712459564209, 0.6190920352935791, 1.160452922185262, 2.196955839792887, 0.43649061520894367,
             2.1116132736206055, 1.0336641470591228, 1.1934993743896485, 3.5158454895019533, 0.7965855121612548,
             1.2764784495035808, 2.3215492248535154, 0.33251188198725384, 0.42706247170766193, 1.2336311340332031,
             0.8767971992492676, 2.585533777872721, 0.17027787367502847, 0.1086230178674062, 1.8704458236694337,
             1.0339374542236328, 0.9024324417114258, 1.3262407779693604, 0.620256225268046, 0.5925985177357992,
             1.9649040222167968, 0.22099602222442627, 0.3327861825625102, 0.12571901082992554, 0.08246222635110219]
    val = [8.194173812866211, 8.219810485839844, 8.07890510559082, 7.999988555908203, 8.109285354614258,
           5.443463643391927, 7.918091773986816, 7.891481876373291, 7.813107490539551, 7.856866836547852,
           3.2456295013427736, 4.100022315979004, 7.789461612701416, 5.5303955078125, 4.1179962158203125,
           7.003884315490723, 3.2398910522460938, 7.179136753082275, 2.10046124458313, 6.920229434967041,
           5.656375885009766, 6.676296710968018, 6.1791205406188965, 7.147122383117676, 5.829642295837402,
           5.817508697509766, 6.223083972930908, 7.122016270955403, 6.91365114847819, 5.610843658447266,
           6.536789417266846, 7.298920440673828, 6.880926132202148, 6.5459543863932295, 6.831403732299805,
           7.619984149932861, 7.58919677734375, 6.02599573135376, 6.216220855712891, 6.530293782552083,
           6.829213460286458, 7.1236467361450195, 7.362570444742839, 8.319687843322754, 7.189105033874512,
           7.936705589294434, 7.277769088745117, 6.139681816101074, 6.813492774963379, 7.6430888175964355,
           7.6251349449157715, 7.57797368367513, 5.49737548828125, 6.162005424499512, 7.190008163452148,
           5.297762680053711, 7.625123977661133, 8.077958424886068, 7.881954669952393, 7.207827091217041,
           8.263684272766113, 7.713072776794434, 7.305417060852051, 7.044993877410889, 7.233814239501953,
           6.649624824523926, 6.61229133605957, 5.652591705322266, 7.319572448730469, 7.680683135986328,
           5.943621063232422, 8.717290242513021, 8.774529774983725, 10.299866994222006, 8.937938690185547,
           4.017017841339111, 7.019591808319092, 7.788059711456299, 7.923825073242187, 8.965201377868652,
           7.119401454925537, 7.096435546875, 8.101513862609863, 8.602184295654297, 7.544288635253906,
           8.292234420776367, 11.102352142333984, 6.884955406188965, 7.392346700032552, 8.783490498860678,
           3.9737706184387207, 8.053571065266928, 9.67480754852295, 7.4211273193359375, 8.921946207682291,
           8.738455772399902, 8.544051170349121, 9.256473541259766, 11.297525787353516, 8.065522193908691]

    x = np.linspace(0, len(train), len(train))
    print(x)

    x = []
    x.extend(range(0, len(train) * 50, 50))
    print(x)
    plt.plot(x, train, label="train")
    plt.plot(x, val, label="val")
    plt.legend()
    plt.show()


self_attention()
